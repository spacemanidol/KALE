modifiers:
  - !EpochRangeModifier
    start_epoch: 0
    end_epoch: 10

training_modifiers:

  - !LearningRateFunctionModifier
    start_epoch: 0.0
    end_epoch: 8.0
    lr_func: cyclic_linear
    cycle_epochs: 2.0
    init_lr: 1e-4
    final_lr: 5e-5

  - !LearningRateFunctionModifier
    start_epoch: 8.0
    end_epoch: 10.0
    lr_func: linear
    init_lr: 1e-4
    final_lr: 1e-6

  - !OBSPruningModifier
    params: [
      "re:roberta.encoder.layer.*.attention.self.query.weight",
      "re:roberta.encoder.layer.*.attention.self.key.weight",
      "re:roberta.encoder.layer.*.attention.self.value.weight",
      "re:roberta.encoder.layer.*.attention.output.dense.weight",
      "re:roberta.encoder.layer.*.intermediate.dense.weight",
      "re:roberta.encoder.layer.*.output.dense.weight",
    ]
    init_sparsity: 0.5
    final_sparsity: 0.85
    start_epoch: 0
    end_epoch: 8
    update_frequency: 2.0
    inter_func: cubic
    global_sparsity: True
    mask_type: unstructured
    num_grads: 1024
    damp: 1e-7
    fisher_block_size: 4
    grad_sampler_kwargs:
      batch_size: 8
