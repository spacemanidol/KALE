modifiers:
    - !EpochRangeModifier
      start_epoch: 0
      end_epoch: 1
  
    - !GMPruningModifier
      params: [
        "re:encoder.layer.*.attention.self.query.weight",
        "re:encoder.layer.*.attention.self.key.weight",
        "re:encoder.layer.*.attention.self.value.weight",
        "re:encoder.layer.*.attention.output.dense.weight",
        "re:encoder.layer.*.intermediate.dense.weight",
        "re:encoder.layer.*.output.dense.weight",
      ]
      init_sparsity: 0.90
      final_sparsity: 0.90
      start_epoch: 0
      end_epoch: 1
      update_frequency: 0.01
      mask_type: unstructured

distillation_modifiers:
 - !QuantizationModifier
        activation_bits: 8
        disable_quantization_observer_epoch: 0.0
        end_epoch: -1.0
        exclude_batchnorm: True
        exclude_module_types: ['LayerNorm', 'Tanh', 'BatchNorm1d', 'BatchNorm2d', 'BatchNorm3d']
        freeze_bn_stats_epoch: 0.00
        model_fuse_fn_name: conv_bn_relus
        quantize_conv_activations: True
        quantize_embeddings: 1   
        quantize_linear_activations: 0
        reduce_range: False
        start_epoch: 0.0
        submodules: ['encoder', 'embeddings']
        tensorrt: False
        weight_bits: 8  