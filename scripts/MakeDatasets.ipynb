{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "animated-visibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/base-none-msmarco:\r\n",
      "config.json\t   corpus_emb.09.pkl  corpus_emb.19.pkl\r\n",
      "corpus_emb.00.pkl  corpus_emb.10.pkl  nearest-neighbor-dev.txt\r\n",
      "corpus_emb.01.pkl  corpus_emb.11.pkl  passage_model\r\n",
      "corpus_emb.02.pkl  corpus_emb.12.pkl  query_emb_dev.pkl\r\n",
      "corpus_emb.03.pkl  corpus_emb.13.pkl  query_emb.pkl\r\n",
      "corpus_emb.04.pkl  corpus_emb.14.pkl  query_emb_train.pkl\r\n",
      "corpus_emb.05.pkl  corpus_emb.15.pkl  query_model\r\n",
      "corpus_emb.06.pkl  corpus_emb.16.pkl  special_tokens_map.json\r\n",
      "corpus_emb.07.pkl  corpus_emb.17.pkl  tokenizer_config.json\r\n",
      "corpus_emb.08.pkl  corpus_emb.18.pkl  vocab.txt\r\n",
      "\r\n",
      "models/base-none-nq:\r\n",
      "config.json\t   corpus_emb.10.pkl\t     nearest-neighbor-eval.txt\r\n",
      "corpus_emb.00.pkl  corpus_emb.11.pkl\t     nearest-neighbor-train.txt\r\n",
      "corpus_emb.01.pkl  corpus_emb.12.pkl\t     passage_model\r\n",
      "corpus_emb.02.pkl  corpus_emb.13.pkl\t     query_emb_dev.pkl\r\n",
      "corpus_emb.03.pkl  corpus_emb.14.pkl\t     query_emb.pkl\r\n",
      "corpus_emb.04.pkl  corpus_emb.15.pkl\t     query_emb_train.pkl\r\n",
      "corpus_emb.05.pkl  corpus_emb.16.pkl\t     query_model\r\n",
      "corpus_emb.06.pkl  corpus_emb.17.pkl\t     special_tokens_map.json\r\n",
      "corpus_emb.07.pkl  corpus_emb.18.pkl\t     tokenizer_config.json\r\n",
      "corpus_emb.08.pkl  corpus_emb.19.pkl\t     tokenizer.json\r\n",
      "corpus_emb.09.pkl  nearest-neighbor-dev.txt  vocab.txt\r\n",
      "\r\n",
      "models/base-none-scifact:\r\n",
      "config.json\t   corpus_emb.10.pkl\t     nearest-neighbor-eval.txt\r\n",
      "corpus_emb.00.pkl  corpus_emb.11.pkl\t     nearest-neighbor-train.txt\r\n",
      "corpus_emb.01.pkl  corpus_emb.12.pkl\t     passage_model\r\n",
      "corpus_emb.02.pkl  corpus_emb.13.pkl\t     query_emb_dev.pkl\r\n",
      "corpus_emb.03.pkl  corpus_emb.14.pkl\t     query_emb.pkl\r\n",
      "corpus_emb.04.pkl  corpus_emb.15.pkl\t     query_emb_train.pkl\r\n",
      "corpus_emb.05.pkl  corpus_emb.16.pkl\t     query_model\r\n",
      "corpus_emb.06.pkl  corpus_emb.17.pkl\t     special_tokens_map.json\r\n",
      "corpus_emb.07.pkl  corpus_emb.18.pkl\t     tokenizer_config.json\r\n",
      "corpus_emb.08.pkl  corpus_emb.19.pkl\t     vocab.txt\r\n",
      "corpus_emb.09.pkl  nearest-neighbor-dev.txt\r\n",
      "\r\n",
      "models/base-none-squad:\r\n",
      "config.json\t\t  nearest-neighbor-train.txt  query_model\r\n",
      "corpus_emb.00.pkl\t  passage_model\t\t      special_tokens_map.json\r\n",
      "corpus_emb.01.pkl\t  pytorch_model.bin\t      tokenizer_config.json\r\n",
      "corpus_emb.02.pkl\t  query_emb_dev.pkl\t      vocab.txt\r\n",
      "corpus_emb.03.pkl\t  query_emb.pkl\r\n",
      "nearest-neighbor-dev.txt  query_emb_train.pkl\r\n",
      "\r\n",
      "models/base-none-trivia:\r\n",
      "config.json\t   corpus_emb.10.pkl\t     nearest-neighbor-eval.txt\r\n",
      "corpus_emb.00.pkl  corpus_emb.11.pkl\t     nearest-neighbor-train.txt\r\n",
      "corpus_emb.01.pkl  corpus_emb.12.pkl\t     passage_model\r\n",
      "corpus_emb.02.pkl  corpus_emb.13.pkl\t     query_emb_dev.pkl\r\n",
      "corpus_emb.03.pkl  corpus_emb.14.pkl\t     query_emb.pkl\r\n",
      "corpus_emb.04.pkl  corpus_emb.15.pkl\t     query_emb_train.pkl\r\n",
      "corpus_emb.05.pkl  corpus_emb.16.pkl\t     query_model\r\n",
      "corpus_emb.06.pkl  corpus_emb.17.pkl\t     special_tokens_map.json\r\n",
      "corpus_emb.07.pkl  corpus_emb.18.pkl\t     tokenizer_config.json\r\n",
      "corpus_emb.08.pkl  corpus_emb.19.pkl\t     vocab.txt\r\n",
      "corpus_emb.09.pkl  nearest-neighbor-dev.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls models/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "drawn-arabic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0\t85.38885498046875\r\n",
      "0\t12564\t75.19364166259766\r\n",
      "0\t2648\t74.8531265258789\r\n",
      "0\t10367\t74.84990692138672\r\n",
      "0\t6329\t74.45631408691406\r\n",
      "0\t58063\t74.15997314453125\r\n",
      "0\t38465\t73.97508239746094\r\n",
      "0\t34046\t73.60050964355469\r\n",
      "0\t56564\t73.34567260742188\r\n",
      "0\t4920\t72.94799041748047\r\n"
     ]
    }
   ],
   "source": [
    "! head models/base-none-nq/nearest-neighbor-train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive, negative, negatives, hard_negative, hard_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "level-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "empirical-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid2query = {}\n",
    "qid2nn = {}\n",
    "with open('datasets/nq/nq-train.jsonl','r') as f:\n",
    "    for l in f:\n",
    "        data = json.loads(l)\n",
    "        qid2query[data['query_id']] = data['query']\n",
    "        \n",
    "with open('models/base-none-nq/nearest-neighbor-train.txt','r') as f:\n",
    "    for l in f:\n",
    "        l = l.strip().split('\\t')\n",
    "        qid = l[0]\n",
    "        target_qid = l[1]\n",
    "        if qid not in qid2nn:\n",
    "            qid2nn[qid] = []\n",
    "        qid2nn[qid].append(target_qid)\n",
    "\n",
    "query_ids = list(qid2query.keys())\n",
    "\n",
    "with open('datasets/nq/nq-train.jsonl','r') as f:\n",
    "    with open('datasets/nq/query_align_train.jsonl','w') as w:\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            query_id = data['query_id']\n",
    "            query_text = data['query']\n",
    "            negatives = []\n",
    "            for i in range(10):\n",
    "                negatives.append(qid2query[random.choice(query_ids)])\n",
    "            hard_negatives = []\n",
    "            for i in range(10):\n",
    "                hard_negatives.append(qid2query[qid2nn[query_id][i]])\n",
    "            data = {'query_id': query_id, 'query_text': query_text, 'negative':negatives[0] , 'negatives':negatives, 'hard_negatives':hard_negatives, 'hard_negative':hard_negatives[0]}\n",
    "            w.write('{}\\n'.format(json.dumps(data)))\n",
    "            \n",
    "qid2query = {}\n",
    "qid2nn = {}\n",
    "            \n",
    "with open('datasets/nq/nq-dev.jsonl','r') as f:\n",
    "    for l in f:\n",
    "        data = json.loads(l)\n",
    "        qid2query[data['query_id']] = data['query']\n",
    "\n",
    "with open('models/base-none-nq/nearest-neighbor-dev.txt','r') as f:\n",
    "    for l in f:\n",
    "        l = l.strip().split('\\t')\n",
    "        qid = l[0]\n",
    "        target_qid = l[1]\n",
    "        if qid not in qid2nn:\n",
    "            qid2nn[qid] = []\n",
    "        qid2nn[qid].append(target_qid)\n",
    "        \n",
    "query_ids = list(qid2query.keys())\n",
    "\n",
    "with open('datasets/nq/nq-dev.jsonl','r') as f:\n",
    "    with open('datasets/nq/query_align_dev.jsonl','w') as w:\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            query_id = data['query_id']\n",
    "            query_text = data['query']\n",
    "            negatives = []\n",
    "            for i in range(10):\n",
    "                negatives.append(qid2query[random.choice(query_ids)])\n",
    "            hard_negatives = []\n",
    "            for i in range(10):\n",
    "                hard_negatives.append(qid2query[qid2nn[query_id][i]])\n",
    "            data = {'query_id': query_id, 'query_text': query_text, 'negative':negatives[0] , 'negatives':negatives, 'hard_negatives':hard_negatives, 'hard_negative':hard_negatives[0]}\n",
    "            w.write('{}\\n'.format(json.dumps(data)))\n",
    "            \n",
    "qid2query = {}\n",
    "qid2nn = {}\n",
    "        \n",
    "with open('datasets/nq/nq-test.jsonl','r') as f:\n",
    "    for l in f:\n",
    "        data = json.loads(l)\n",
    "        qid2query[data['query_id']] = data['query']\n",
    "\n",
    "with open('models/base-none-nq/nearest-neighbor-eval.txt','r') as f:\n",
    "    for l in f:\n",
    "        l = l.strip().split('\\t')\n",
    "        qid = l[0]\n",
    "        target_qid = l[1]\n",
    "        if qid not in qid2nn:\n",
    "            qid2nn[qid] = []\n",
    "        qid2nn[qid].append(target_qid)\n",
    "\n",
    "query_ids = list(qid2query.keys())\n",
    "\n",
    "with open('datasets/nq/nq-test.jsonl','r') as f:\n",
    "    with open('datasets/nq/query_align_test.jsonl','w') as w:\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            query_id = data['query_id']\n",
    "            query_text = data['query']\n",
    "            negatives = []\n",
    "            for i in range(10):\n",
    "                negatives.append(qid2query[random.choice(query_ids)])\n",
    "            hard_negatives = []\n",
    "            for i in range(10):\n",
    "                hard_negatives.append(qid2query[qid2nn[query_id][i]])\n",
    "            data = {'query_id': query_id, 'query_text': query_text, 'negative':negatives[0] , 'negatives':negatives, 'hard_negatives':hard_negatives, 'hard_negative':hard_negatives[0]}\n",
    "            w.write('{}\\n'.format(json.dumps(data)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "persistent-consent",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid2query = {}\n",
    "qid2nn = {}\n",
    "with open('datasets/trivia/trivia-train.jsonl','r') as f:\n",
    "    for l in f:\n",
    "        data = json.loads(l)\n",
    "        qid2query[data['query_id']] = data['query']\n",
    "        \n",
    "with open('models/base-none-trivia/nearest-neighbor-train.txt','r') as f:\n",
    "    for l in f:\n",
    "        l = l.strip().split('\\t')\n",
    "        qid = l[0]\n",
    "        target_qid = l[1]\n",
    "        if qid not in qid2nn:\n",
    "            qid2nn[qid] = []\n",
    "        qid2nn[qid].append(target_qid)\n",
    "\n",
    "query_ids = list(qid2query.keys())\n",
    "\n",
    "with open('datasets/trivia/trivia-train.jsonl','r') as f:\n",
    "    with open('datasets/trivia/query_align_train.jsonl','w') as w:\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            query_id = data['query_id']\n",
    "            query_text = data['query']\n",
    "            negatives = []\n",
    "            for i in range(10):\n",
    "                negatives.append(qid2query[random.choice(query_ids)])\n",
    "            hard_negatives = []\n",
    "            for i in range(10):\n",
    "                hard_negatives.append(qid2query[qid2nn[query_id][i]])\n",
    "            data = {'query_id': query_id, 'query_text': query_text, 'negative':negatives[0] , 'negatives':negatives, 'hard_negatives':hard_negatives, 'hard_negative':hard_negatives[0]}\n",
    "            w.write('{}\\n'.format(json.dumps(data)))\n",
    "            \n",
    "qid2query = {}\n",
    "qid2nn = {}\n",
    "            \n",
    "with open('datasets/trivia/trivia-dev.jsonl','r') as f:\n",
    "    for l in f:\n",
    "        data = json.loads(l)\n",
    "        qid2query[data['query_id']] = data['query']\n",
    "\n",
    "with open('models/base-none-trivia/nearest-neighbor-dev.txt','r') as f:\n",
    "    for l in f:\n",
    "        l = l.strip().split('\\t')\n",
    "        qid = l[0]\n",
    "        target_qid = l[1]\n",
    "        if qid not in qid2nn:\n",
    "            qid2nn[qid] = []\n",
    "        qid2nn[qid].append(target_qid)\n",
    "        \n",
    "query_ids = list(qid2query.keys())\n",
    "\n",
    "with open('datasets/trivia/trivia-dev.jsonl','r') as f:\n",
    "    with open('datasets/trivia/query_align_dev.jsonl','w') as w:\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            query_id = data['query_id']\n",
    "            query_text = data['query']\n",
    "            negatives = []\n",
    "            for i in range(10):\n",
    "                negatives.append(qid2query[random.choice(query_ids)])\n",
    "            hard_negatives = []\n",
    "            for i in range(10):\n",
    "                hard_negatives.append(qid2query[qid2nn[query_id][i]])\n",
    "            data = {'query_id': query_id, 'query_text': query_text, 'negative':negatives[0] , 'negatives':negatives, 'hard_negatives':hard_negatives, 'hard_negative':hard_negatives[0]}\n",
    "            w.write('{}\\n'.format(json.dumps(data)))\n",
    "            \n",
    "qid2query = {}\n",
    "qid2nn = {}\n",
    "        \n",
    "with open('datasets/trivia/trivia-test.jsonl','r') as f:\n",
    "    for l in f:\n",
    "        data = json.loads(l)\n",
    "        qid2query[data['query_id']] = data['query']\n",
    "\n",
    "with open('models/base-none-trivia/nearest-neighbor-eval.txt','r') as f:\n",
    "    for l in f:\n",
    "        l = l.strip().split('\\t')\n",
    "        qid = l[0]\n",
    "        target_qid = l[1]\n",
    "        if qid not in qid2nn:\n",
    "            qid2nn[qid] = []\n",
    "        qid2nn[qid].append(target_qid)\n",
    "\n",
    "query_ids = list(qid2query.keys())\n",
    "\n",
    "with open('datasets/trivia/trivia-test.jsonl','r') as f:\n",
    "    with open('datasets/trivia/query_align_test.jsonl','w') as w:\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            query_id = data['query_id']\n",
    "            query_text = data['query']\n",
    "            negatives = []\n",
    "            for i in range(10):\n",
    "                negatives.append(qid2query[random.choice(query_ids)])\n",
    "            hard_negatives = []\n",
    "            for i in range(10):\n",
    "                hard_negatives.append(qid2query[qid2nn[query_id][i]])\n",
    "            data = {'query_id': query_id, 'query_text': query_text, 'negative':negatives[0] , 'negatives':negatives, 'hard_negatives':hard_negatives, 'hard_negative':hard_negatives[0]}\n",
    "            w.write('{}\\n'.format(json.dumps(data)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "distinct-intervention",
   "metadata": {},
   "outputs": [],
   "source": [
    "orcas_queries = []\n",
    "with open('datasets/orcas-doctrain-queries.tsv','r') as f:\n",
    "    for l in f:\n",
    "        l = l.strip().split('\\t')\n",
    "        if len(l) >1:\n",
    "            orcas_queries.append(l[1])\n",
    "random.shuffle(orcas_queries)\n",
    "orcas_train = orcas_queries[:int(len(orcas_queries)*0.9)]\n",
    "orcas_dev = orcas_queries[int(len(orcas_queries)*0.9):int(len(orcas_queries)*0.95)]\n",
    "orcas_eval = orcas_queries[int(len(orcas_queries)*0.95):]\n",
    "print(len(orcas_train))\n",
    "print(len(orcas_dev))\n",
    "print(len(orcas_eval))\n",
    "\n",
    "i = 0\n",
    "with open('datasets/orcas/query_align_train.jsonl','w') as w:\n",
    "    for query in orcas_train:\n",
    "        negatives = []\n",
    "        for i in range(10):\n",
    "            negatives.append(random.choice(orcas_train))\n",
    "        i += 1\n",
    "        data = {'query_id': i, 'query_text': query, 'negative':negatives[0] , 'negatives':negatives, 'hard_negatives':negatives, 'hard_negative':negatives[0]}\n",
    "        w.write('{}\\n'.format(json.dumps(data)))\n",
    "\n",
    "i = 0\n",
    "with open('datasets/orcas/query_align_dev.jsonl','w') as w:\n",
    "    for query in orcas_dev:\n",
    "        negatives = []\n",
    "        for i in range(10):\n",
    "            negatives.append(random.choice(orcas_dev))\n",
    "        i += 1\n",
    "        data = {'query_id': i, 'query_text': query, 'negative':negatives[0] , 'negatives':negatives, 'hard_negatives':negatives, 'hard_negative':negatives[0]}\n",
    "        w.write('{}\\n'.format(json.dumps(data)))\n",
    "        \n",
    "i = 0\n",
    "with open('datasets/orcas/query_align_eval.jsonl','w') as w:\n",
    "    for query in orcas_eval:\n",
    "        negatives = []\n",
    "        for i in range(10):\n",
    "            negatives.append(random.choice(orcas_eval))\n",
    "        i += 1\n",
    "        data = {'query_id': i, 'query_text': query, 'negative':negatives[0] , 'negatives':negatives, 'hard_negatives':negatives, 'hard_negative':negatives[0]}\n",
    "        w.write('{}\\n'.format(json.dumps(data)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "statistical-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "with open('datasets/orcas/query_align_train.jsonl','w') as w:\n",
    "    for query in orcas_train:\n",
    "        negatives = []\n",
    "        for i in range(10):\n",
    "            negatives.append(random.choice(orcas_train))\n",
    "        i += 1\n",
    "        data = {'query_id': i, 'query_text': query, 'negative':negatives[0] , 'negatives':negatives, 'hard_negatives':negatives, 'hard_negative':negatives[0]}\n",
    "        w.write('{}\\n'.format(json.dumps(data)))\n",
    "\n",
    "i = 0\n",
    "with open('datasets/orcas/query_align_dev.jsonl','w') as w:\n",
    "    for query in orcas_dev:\n",
    "        negatives = []\n",
    "        for i in range(10):\n",
    "            negatives.append(random.choice(orcas_dev))\n",
    "        i += 1\n",
    "        data = {'query_id': i, 'query_text': query, 'negative':negatives[0] , 'negatives':negatives, 'hard_negatives':negatives, 'hard_negative':negatives[0]}\n",
    "        w.write('{}\\n'.format(json.dumps(data)))\n",
    "        \n",
    "i = 0\n",
    "with open('datasets/orcas/query_align_eval.jsonl','w') as w:\n",
    "    for query in orcas_eval:\n",
    "        negatives = []\n",
    "        for i in range(10):\n",
    "            negatives.append(random.choice(orcas_eval))\n",
    "        i += 1\n",
    "        data = {'query_id': i, 'query_text': query, 'negative':negatives[0] , 'negatives':negatives, 'hard_negatives':negatives, 'hard_negative':negatives[0]}\n",
    "        w.write('{}\\n'.format(json.dumps(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "registered-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid2query = {}\n",
    "qid2nn = {}\n",
    "with open('datasets/squad/squad-train.jsonl','r') as f:\n",
    "    for l in f:\n",
    "        data = json.loads(l)\n",
    "        qid2query[data['query_id']] = data['query']\n",
    "        \n",
    "with open('models/base-none-squad/nearest-neighbor-train.txt','r') as f:\n",
    "    for l in f:\n",
    "        l = l.strip().split('\\t')\n",
    "        qid = l[0]\n",
    "        target_qid = l[1]\n",
    "        if qid not in qid2nn:\n",
    "            qid2nn[qid] = []\n",
    "        qid2nn[qid].append(target_qid)\n",
    "\n",
    "query_ids = list(qid2query.keys())\n",
    "\n",
    "with open('datasets/squad/squad-train.jsonl','r') as f:\n",
    "    with open('datasets/squad/query_align_train.jsonl','w') as w:\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            query_id = data['query_id']\n",
    "            query_text = data['query']\n",
    "            negatives = []\n",
    "            for i in range(10):\n",
    "                negatives.append(qid2query[random.choice(query_ids)])\n",
    "            hard_negatives = []\n",
    "            for i in range(10):\n",
    "                hard_negatives.append(qid2query[qid2nn[query_id][i]])\n",
    "            data = {'query_id': query_id, 'query_text': query_text, 'negative':negatives[0] , 'negatives':negatives, 'hard_negatives':hard_negatives, 'hard_negative':hard_negatives[0]}\n",
    "            w.write('{}\\n'.format(json.dumps(data)))\n",
    "            \n",
    "qid2query = {}\n",
    "qid2nn = {}\n",
    "            \n",
    "with open('datasets/squad/squad-dev.jsonl','r') as f:\n",
    "    for l in f:\n",
    "        data = json.loads(l)\n",
    "        qid2query[data['query_id']] = data['query']\n",
    "\n",
    "with open('models/base-none-squad/nearest-neighbor-dev.txt','r') as f:\n",
    "    for l in f:\n",
    "        l = l.strip().split('\\t')\n",
    "        qid = l[0]\n",
    "        target_qid = l[1]\n",
    "        if qid not in qid2nn:\n",
    "            qid2nn[qid] = []\n",
    "        qid2nn[qid].append(target_qid)\n",
    "        \n",
    "query_ids = list(qid2query.keys())\n",
    "\n",
    "with open('datasets/squad/squad-dev.jsonl','r') as f:\n",
    "    with open('datasets/squad/query_align_dev.jsonl','w') as w:\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            query_id = data['query_id']\n",
    "            query_text = data['query']\n",
    "            negatives = []\n",
    "            for i in range(10):\n",
    "                negatives.append(qid2query[random.choice(query_ids)])\n",
    "            hard_negatives = []\n",
    "            for i in range(10):\n",
    "                hard_negatives.append(qid2query[qid2nn[query_id][i]])\n",
    "            data = {'query_id': query_id, 'query_text': query_text, 'negative':negatives[0] , 'negatives':negatives, 'hard_negatives':hard_negatives, 'hard_negative':hard_negatives[0]}\n",
    "            w.write('{}\\n'.format(json.dumps(data)))\n",
    "            \n",
    "qid2query = {}\n",
    "qid2nn = {}\n",
    "        \n",
    "with open('datasets/squad/squad-test.jsonl','r') as f:\n",
    "    for l in f:\n",
    "        data = json.loads(l)\n",
    "        qid2query[data['query_id']] = data['query']\n",
    "\n",
    "with open('models/base-none-squad/nearest-neighbor-eval.txt','r') as f:\n",
    "    for l in f:\n",
    "        l = l.strip().split('\\t')\n",
    "        qid = l[0]\n",
    "        target_qid = l[1]\n",
    "        if qid not in qid2nn:\n",
    "            qid2nn[qid] = []\n",
    "        qid2nn[qid].append(target_qid)\n",
    "\n",
    "query_ids = list(qid2query.keys())\n",
    "\n",
    "with open('datasets/squad/squad-test.jsonl','r') as f:\n",
    "    with open('datasets/squad/query_align_test.jsonl','w') as w:\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            query_id = data['query_id']\n",
    "            query_text = data['query']\n",
    "            negatives = []\n",
    "            for i in range(10):\n",
    "                negatives.append(qid2query[random.choice(query_ids)])\n",
    "            hard_negatives = []\n",
    "            for i in range(10):\n",
    "                hard_negatives.append(qid2query[qid2nn[query_id][i]])\n",
    "            data = {'query_id': query_id, 'query_text': query_text, 'negative':negatives[0] , 'negatives':negatives, 'hard_negatives':hard_negatives, 'hard_negative':hard_negatives[0]}\n",
    "            w.write('{}\\n'.format(json.dumps(data)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "governing-charter",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid2query = {}\n",
    "qid2nn = {}\n",
    "with open('datasets/msmarco/train.jsonl','r') as f:\n",
    "    for l in f:\n",
    "        data = json.loads(l)\n",
    "        qid2query[data['query_id']] = data['query']\n",
    "        \n",
    "with open('models/base-none-msmarco/nearest-neighbor-train.txt','r') as f:\n",
    "    for l in f:\n",
    "        l = l.strip().split('\\t')\n",
    "        qid = l[0]\n",
    "        target_qid = l[1]\n",
    "        if qid not in qid2nn:\n",
    "            qid2nn[qid] = []\n",
    "        qid2nn[qid].append(target_qid)\n",
    "\n",
    "query_ids = list(qid2query.keys())\n",
    "\n",
    "with open('datasets/msmarco/train.jsonl','r') as f:\n",
    "    with open('datasets/msmarco/query_align_train.jsonl','w') as w:\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            query_id = data['query_id']\n",
    "            query_text = data['query']\n",
    "            negatives = []\n",
    "            for i in range(10):\n",
    "                negatives.append(qid2query[random.choice(query_ids)])\n",
    "            hard_negatives = []\n",
    "            for i in range(10):\n",
    "                hard_negatives.append(qid2query[qid2nn[query_id][i]])\n",
    "            data = {'query_id': query_id, 'query_text': query_text, 'negative':negatives[0] , 'negatives':negatives, 'hard_negatives':hard_negatives, 'hard_negative':hard_negatives[0]}\n",
    "            w.write('{}\\n'.format(json.dumps(data)))\n",
    "            \n",
    "qid2query = {}\n",
    "qid2nn = {}\n",
    "            \n",
    "with open('datasets/msmarco/dev.jsonl','r') as f:\n",
    "    for l in f:\n",
    "        data = json.loads(l)\n",
    "        qid2query[data['query_id']] = data['query']\n",
    "\n",
    "with open('models/base-none-msmarco/nearest-neighbor-dev.txt','r') as f:\n",
    "    for l in f:\n",
    "        l = l.strip().split('\\t')\n",
    "        qid = l[0]\n",
    "        target_qid = l[1]\n",
    "        if qid not in qid2nn:\n",
    "            qid2nn[qid] = []\n",
    "        qid2nn[qid].append(target_qid)\n",
    "        \n",
    "query_ids = list(qid2query.keys())\n",
    "\n",
    "with open('datasets/msmarco/dev.jsonl','r') as f:\n",
    "    with open('datasets/msmarco/query_align_dev.jsonl','w') as w:\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            query_id = data['query_id']\n",
    "            query_text = data['query']\n",
    "            negatives = []\n",
    "            for i in range(10):\n",
    "                negatives.append(qid2query[random.choice(query_ids)])\n",
    "            hard_negatives = []\n",
    "            for i in range(10):\n",
    "                hard_negatives.append(qid2query[qid2nn[query_id][i]])\n",
    "            data = {'query_id': query_id, 'query_text': query_text, 'negative':negatives[0] , 'negatives':negatives, 'hard_negatives':hard_negatives, 'hard_negative':hard_negatives[0]}\n",
    "            w.write('{}\\n'.format(json.dumps(data)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "sporting-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid2query = {}\n",
    "qid2nn = {}\n",
    "with open('datasets/scifact/train.jsonl','r') as f:\n",
    "    for l in f:\n",
    "        data = json.loads(l)\n",
    "        qid2query[data['query_id']] = data['query']\n",
    "        \n",
    "with open('models/base-none-scifact/nearest-neighbor-train.txt','r') as f:\n",
    "    for l in f:\n",
    "        l = l.strip().split('\\t')\n",
    "        qid = l[0]\n",
    "        target_qid = l[1]\n",
    "        if qid not in qid2nn:\n",
    "            qid2nn[qid] = []\n",
    "        qid2nn[qid].append(target_qid)\n",
    "\n",
    "query_ids = list(qid2query.keys())\n",
    "\n",
    "with open('datasets/scifact/train.jsonl','r') as f:\n",
    "    with open('datasets/scifact/query_align_train.jsonl','w') as w:\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            query_id = data['query_id']\n",
    "            query_text = data['query']\n",
    "            negatives = []\n",
    "            for i in range(10):\n",
    "                negatives.append(qid2query[random.choice(query_ids)])\n",
    "            hard_negatives = []\n",
    "            for i in range(10):\n",
    "                hard_negatives.append(qid2query[qid2nn[query_id][i]])\n",
    "            data = {'query_id': query_id, 'query_text': query_text, 'negative':negatives[0] , 'negatives':negatives, 'hard_negatives':hard_negatives, 'hard_negative':hard_negatives[0]}\n",
    "            w.write('{}\\n'.format(json.dumps(data)))\n",
    "            \n",
    "qid2query = {}\n",
    "qid2nn = {}\n",
    "            \n",
    "with open('datasets/scifact/dev.jsonl','r') as f:\n",
    "    for l in f:\n",
    "        data = json.loads(l)\n",
    "        qid2query[data['query_id']] = data['query']\n",
    "\n",
    "with open('models/base-none-scifact/nearest-neighbor-dev.txt','r') as f:\n",
    "    for l in f:\n",
    "        l = l.strip().split('\\t')\n",
    "        qid = l[0]\n",
    "        target_qid = l[1]\n",
    "        if qid not in qid2nn:\n",
    "            qid2nn[qid] = []\n",
    "        qid2nn[qid].append(target_qid)\n",
    "        \n",
    "query_ids = list(qid2query.keys())\n",
    "\n",
    "with open('datasets/scifact/dev.jsonl','r') as f:\n",
    "    with open('datasets/scifact/query_align_dev.jsonl','w') as w:\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            query_id = data['query_id']\n",
    "            query_text = data['query']\n",
    "            negatives = []\n",
    "            for i in range(10):\n",
    "                negatives.append(qid2query[random.choice(query_ids)])\n",
    "            hard_negatives = []\n",
    "            for i in range(10):\n",
    "                hard_negatives.append(qid2query[qid2nn[query_id][i]])\n",
    "            data = {'query_id': query_id, 'query_text': query_text, 'negative':negatives[0] , 'negatives':negatives, 'hard_negatives':hard_negatives, 'hard_negative':hard_negatives[0]}\n",
    "            w.write('{}\\n'.format(json.dumps(data)))\n",
    "            \n",
    "qid2query = {}\n",
    "qid2nn = {}\n",
    "        \n",
    "with open('datasets/scifact/test.jsonl','r') as f:\n",
    "    for l in f:\n",
    "        data = json.loads(l)\n",
    "        qid2query[data['query_id']] = data['query']\n",
    "\n",
    "with open('models/base-none-scifact/nearest-neighbor-eval.txt','r') as f:\n",
    "    for l in f:\n",
    "        l = l.strip().split('\\t')\n",
    "        qid = l[0]\n",
    "        target_qid = l[1]\n",
    "        if qid not in qid2nn:\n",
    "            qid2nn[qid] = []\n",
    "        qid2nn[qid].append(target_qid)\n",
    "\n",
    "query_ids = list(qid2query.keys())\n",
    "\n",
    "with open('datasets/scifact/test.jsonl','r') as f:\n",
    "    with open('datasets/scifact/query_align_test.jsonl','w') as w:\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            query_id = data['query_id']\n",
    "            query_text = data['query']\n",
    "            negatives = []\n",
    "            for i in range(10):\n",
    "                negatives.append(qid2query[random.choice(query_ids)])\n",
    "            hard_negatives = []\n",
    "            for i in range(10):\n",
    "                hard_negatives.append(qid2query[qid2nn[query_id][i]])\n",
    "            data = {'query_id': query_id, 'query_text': query_text, 'negative':negatives[0] , 'negatives':negatives, 'hard_negatives':hard_negatives, 'hard_negative':hard_negatives[0]}\n",
    "            w.write('{}\\n'.format(json.dumps(data)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "arbitrary-routine",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/orcas-doctrain-queries.tsv','r') as f:\n",
    "    with open('train.jsonl','w') as w:\n",
    "        for l in f:\n",
    "            l = l.strip().split('\\t')\n",
    "            if len(l) > 1:\n",
    "                data = {'query_id': l[0], 'query':l[1]}\n",
    "                w.write(\"{}\\n\".format(json.dumps(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "rural-handy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'big little lies season 2 how many episodes'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qid2query_nq = {}\n",
    "with open('datasets/nq/nq-train.jsonl','r') as f:\n",
    "    for l in f:\n",
    "        data = json.loads(l)\n",
    "        qid2query_nq[data['query_id']] = data['query']\n",
    "\n",
    "        \n",
    "with open('datasets/nq/nq-dev.jsonl','r') as f:\n",
    "    for l in f:\n",
    "        data = json.loads(l)\n",
    "        qid2query_nq[data['query_id']] = data['query']\n",
    "\n",
    "        \n",
    "with open('datasets/nq/nq-test.jsonl','r') as f:\n",
    "    for l in f:\n",
    "        data = json.loads(l)\n",
    "        qid2query_nq[data['query_id']] = data['query']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-publication",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
